{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q0. What is the difference between `shallow neural network` and `deep neural networks`?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have seen two feature extraction techniques for text data -\n",
    "- Bag of Words\n",
    "- TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But apart from this we can also use below feature extraction techniques for texts - \n",
    "- One hot encoding\n",
    "- Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume I have word dictionary as shown below -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE1.png 'word dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE2.png 'one hot vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I have sentences and if I use `one-hot encoding` as feature extraction, the sentences would like this -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE3.png 'one hot representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q1. What are some disadvantages of using `one hot vectors`?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer - \n",
    "- It creates a very large vector for the text or it creates a very `sparse matrix` which takes lot memory and computational time\n",
    "- No embedded meaning - that is does not carry word's meaning as shown below - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE4.png 'disadvantage of one hot representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE5.png 'word vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarly we can keep adding more dimensions to these above word vectors and they will be able to capture the `semantic` relationships between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q2. What are the advantages of using `word vectors`?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer - \n",
    "- It is low dimensional than one hot vectors\n",
    "- word vectors able to capture the meaning of the words or the semantic relationships among the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE6.png 'Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1\\. Corpus\n",
    "    * The corpus should have context in itself. For example, if we want to create word embedding for legal/law based clients then I should have corpus related to law books, legal contracts etc. A wikipedia corpus will not help to capture the context which are related to legal/law based business cases\n",
    "* 2\\. Embedding method\n",
    "    * Word embeddings are generally `by product` of a `semi-supervised` machine/deep learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word2Vec** (you can read the research paper [here](https://arxiv.org/pdf/1301.3781.pdf)). It uses `shallow neural network` to train the word vectors and it has two model architectures - \n",
    "    - **Continuous bag of words (COBW)**: This architecture tried to learn a missing word given its surrounding words\n",
    "    - **Continuous skip-gram/skip-gram with negative sampling (SGNS)**: This architecture tries to learn the surrounding words given an input word\n",
    "    \n",
    "- **Global vectors (GloVe)**, you can read the research paper [here](https://nlp.stanford.edu/pubs/glove.pdf). It generates the word vectors by factorizing the logarithm of corpus's word co-occurence matrix.\n",
    "\n",
    "- **fastText** (you can read the research paper [here](https://arxiv.org/pdf/1607.04606.pdf)). It also uses skip-gram architectures and represents words as n-gram of characters. There is another embedding from facebook - [StarSpace](https://arxiv.org/pdf/1709.03856.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The above word vectors will have same word vectors no matter what in what context they are being used. But there are few advanced word embedding models where the words changes their embeddings based on the context on where they are being used. Below are some examples - </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embeddings from language models (ELMo)**, you can read the research paper [here](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "- **Bidirectinal encoder representation from transformers (BERT)**, you can read the reseach paper [here](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- **Generative pre-training 3 (GPT 3)**, you can read the research paper [here](https://arxiv.org/pdf/2005.14165.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>You can finetune these above three models to have customized word embeddings based on the corpus of your business problem that you are solving</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous bag of words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this architecture we try to predict the center word based on its surrouding words. The idea is - if two uniques words are frequently surrounded by similar sets of words when used in various sentences then those words tend to be related in their meaning or in other words we can say that those two words are `semantically` similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE7.png 'skip-gram intution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a large enough corpus the model will learn to predict that the missing word is - `reading`, `studying` or `writing` and some synonyms of these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE8.png 'skip-gram intution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE9.png 'skip-gram architecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q3. How can we prepare the above data for training? As we already know machines don't understand string</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE10.png 'skip-gram architecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to extract the `word embeddings` from above network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE11.png 'extracting word embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe word embeddings creates `co-occurence` matrices and then it used `matrix factorization` technique to compute word embeddings. There are two types co-occurence matrices - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first order co-occurences (bee and honey)\n",
    "- second order co-occurences (bee and bumblebee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE12.png 'co-occurence matrices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to compute the above blue values, we can -\n",
    "- keep a window size of our choice (this is a hyper parameter)\n",
    "- compute word co-occurences: n_uv\n",
    "- or we can compute pointwise mutual information (PMI)\n",
    "$$PMI = log\\frac{P(u, v)}{P(u) * P(v)}$$\n",
    "this measure helps to remove those words which are co-occured randomly and they actually don't have any correlation among them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE13.png 'matrix factorization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q3. But how GloVe factorizez the PMI matrix?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe uses `truncated SVD` technique to factorize the PMI matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE14.png 'SVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the total number of `eigen values` will be equal to the `rank` of the PMI matrix. If rank is smaller than the size of the matrix then we will get `size-rank` number of zero eigen values or if rank is equal to size of the matrix then the last few eigen values are very small and does not actually convey any important meaning, so we can ignore those zero/smaller eigen values. Therefore, we can use `truncated SVD` to remove the unnecessary/meaningless information from our matrix factorization. But how to truncate these matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE15.png 'Truncated SVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies and biases of word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE16.png 'word analogies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors might have `biases` because of the inherent biases present in the training data itself. You can read more about word vectors biases and how to debias them in this [paper](https://arxiv.org/pdf/1607.06520.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](WE17.png 'word vector biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00772095,  0.09619141,  0.03125   ,  0.26953125, -0.07080078,\n",
       "       -0.03686523,  0.09960938, -0.06591797, -0.18945312,  0.06396484,\n",
       "       -0.05029297, -0.13085938,  0.02905273,  0.2109375 , -0.13574219,\n",
       "        0.00878906, -0.02661133, -0.04248047, -0.01489258, -0.02026367,\n",
       "        0.0559082 ,  0.04858398,  0.00350952, -0.06835938, -0.04418945,\n",
       "       -0.15234375, -0.10253906, -0.07519531,  0.02307129, -0.07080078,\n",
       "       -0.11035156,  0.10205078,  0.02062988,  0.01452637, -0.11132812,\n",
       "       -0.16699219,  0.11425781, -0.01989746,  0.05908203, -0.09912109,\n",
       "       -0.03015137, -0.05102539,  0.03710938,  0.06176758, -0.05932617,\n",
       "       -0.25195312, -0.1640625 , -0.01324463,  0.06982422, -0.02929688,\n",
       "        0.03295898,  0.12890625,  0.0112915 ,  0.11328125,  0.13183594,\n",
       "        0.02514648, -0.06494141, -0.09130859, -0.02270508, -0.20800781,\n",
       "       -0.05859375, -0.04541016, -0.08544922, -0.08544922,  0.09228516,\n",
       "       -0.03613281, -0.17871094,  0.04467773,  0.07080078, -0.01867676,\n",
       "       -0.04589844,  0.07910156,  0.08251953,  0.09765625, -0.02368164,\n",
       "       -0.19726562, -0.10693359, -0.2265625 , -0.02209473,  0.08203125,\n",
       "        0.05664062,  0.01953125, -0.0222168 , -0.03808594, -0.01794434,\n",
       "        0.14550781, -0.01647949,  0.04125977,  0.17382812,  0.10742188,\n",
       "        0.02001953, -0.03149414, -0.03833008,  0.05957031, -0.0534668 ,\n",
       "        0.05395508,  0.05517578, -0.0291748 ,  0.11230469, -0.0255127 ,\n",
       "       -0.04125977, -0.06005859,  0.11328125, -0.01586914, -0.03198242,\n",
       "       -0.1484375 ,  0.11816406,  0.14355469,  0.12988281, -0.03735352,\n",
       "       -0.01464844, -0.04541016,  0.15722656, -0.13574219,  0.15234375,\n",
       "        0.09814453, -0.0100708 , -0.21777344,  0.14648438, -0.04418945,\n",
       "        0.07324219,  0.14257812, -0.10253906, -0.05615234,  0.07275391,\n",
       "        0.06054688, -0.06396484, -0.02441406,  0.12695312,  0.04736328,\n",
       "       -0.12158203,  0.05761719,  0.08789062, -0.1640625 ,  0.06787109,\n",
       "       -0.0043335 ,  0.01879883, -0.02514648,  0.07373047,  0.02429199,\n",
       "       -0.00817871, -0.09082031, -0.07080078,  0.05029297, -0.0072937 ,\n",
       "        0.22167969,  0.1484375 , -0.00411987, -0.03857422, -0.08837891,\n",
       "        0.17675781, -0.08984375,  0.04199219, -0.06298828,  0.10253906,\n",
       "       -0.20117188,  0.03051758,  0.01489258,  0.09570312, -0.02355957,\n",
       "       -0.09863281, -0.12695312,  0.03112793,  0.04614258,  0.04589844,\n",
       "       -0.05151367,  0.06494141, -0.00939941, -0.12890625, -0.05932617,\n",
       "       -0.20703125,  0.06591797,  0.06835938,  0.00108337,  0.01055908,\n",
       "       -0.02502441,  0.10351562, -0.14746094,  0.0480957 , -0.19335938,\n",
       "       -0.04248047,  0.0612793 ,  0.01904297, -0.05639648,  0.1328125 ,\n",
       "       -0.05126953,  0.01782227, -0.04736328,  0.13183594,  0.06933594,\n",
       "       -0.19335938,  0.0279541 , -0.12988281,  0.01806641,  0.02453613,\n",
       "       -0.11914062,  0.03515625,  0.03881836, -0.04467773,  0.03881836,\n",
       "       -0.03564453, -0.05737305, -0.11962891, -0.06347656,  0.06347656,\n",
       "       -0.13183594,  0.22363281,  0.00265503, -0.05297852,  0.03222656,\n",
       "       -0.16015625, -0.13476562, -0.10400391, -0.24707031,  0.04052734,\n",
       "       -0.03515625,  0.07470703,  0.08837891, -0.05834961, -0.12792969,\n",
       "        0.04174805,  0.00628662, -0.03735352,  0.00179291, -0.05981445,\n",
       "       -0.03930664, -0.07617188, -0.05224609,  0.07617188,  0.08300781,\n",
       "        0.01416016,  0.04296875, -0.1328125 ,  0.11328125,  0.21289062,\n",
       "       -0.07324219,  0.11767578, -0.03491211, -0.05688477,  0.15820312,\n",
       "        0.1484375 , -0.08203125, -0.06982422,  0.09277344, -0.04370117,\n",
       "       -0.15527344, -0.04736328,  0.07226562, -0.00069427,  0.06494141,\n",
       "        0.02746582,  0.11816406,  0.20214844,  0.02453613,  0.03857422,\n",
       "       -0.07080078, -0.08398438, -0.09130859, -0.10009766, -0.01165771,\n",
       "        0.00521851,  0.06689453, -0.03222656, -0.16015625, -0.06542969,\n",
       "        0.07226562,  0.05517578,  0.06201172,  0.08496094, -0.00842285,\n",
       "       -0.00527954,  0.03710938,  0.05126953,  0.04736328,  0.27539062,\n",
       "       -0.04174805, -0.06835938, -0.04663086, -0.06787109,  0.06787109,\n",
       "       -0.0135498 ,  0.07958984, -0.11230469,  0.00964355,  0.04663086,\n",
       "        0.05322266, -0.02380371,  0.06640625, -0.18554688,  0.23046875,\n",
       "        0.00098419, -0.12158203, -0.078125  , -0.04150391,  0.125     ,\n",
       "        0.07324219, -0.09814453, -0.01324463, -0.06982422,  0.12695312],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings['anjali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
