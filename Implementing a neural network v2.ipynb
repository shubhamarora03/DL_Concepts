{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('iris.data', header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Iris-setosa        50\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['species'] != 'Iris-virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-versicolor    50\n",
       "Iris-setosa        50\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['species'] = data['species'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4)\n",
      "(80,)\n",
      "(20, 4)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(['species'], axis=1), data['species'], test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image_3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Initialize parameters</li>\n",
    "<li>Compute score (the linear function)</li>\n",
    "<li>Compute activation using the score</li>\n",
    "<li>Complete the entire forward propagation step</li>\n",
    "<li>Complete cost/error</li>\n",
    "<li>Complete gradients</li>\n",
    "<li>Update the initialized weights</li>\n",
    "<li>Complete the entire back propagation step</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetry problem in gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539  0.01744812 -0.00761207]\n",
      " [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00322417 -0.00384054  0.01133769]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(4, 3, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing score (linear function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation i.e. computing the score.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of data points)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Why are we caching A, W and be here?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.53646455],\n",
       "       [1.23114741],\n",
       "       [0.3347684 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A = np.random.randn(4, 1)\n",
    "W = np.random.randn(3, 4)\n",
    "b = np.random.randn(3, 1)\n",
    "compute_score(A, W, b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_activation(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = compute_score(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = compute_score(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.      ],\n",
       "       [4.248886],\n",
       "       [0.      ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "A_prev = np.random.randn(4, 1)\n",
    "W = np.random.randn(3, 4)\n",
    "b = np.random.randn(3, 1)\n",
    "apply_activation(A_prev, W, b, activation='relu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of observations)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of apply_activation()\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = apply_activation(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = apply_activation(A, parameters['W'+str(l+1)], parameters['b'+str(l+1)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73736167, 0.6345208 , 0.58031612, 0.5709155 , 0.78774671,\n",
       "        0.68975345, 0.62698738, 0.76990849, 0.980209  , 0.74654404]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(4,10)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "            \"b1\": b1,\n",
    "            \"W2\": W2,\n",
    "            \"b2\": b2}\n",
    "\n",
    "forward_propagation(X, parameters)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost/error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(y_{hat}^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- y_{hat}^{(i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by above equation.\n",
    "\n",
    "    Arguments:\n",
    "    AL, y_hat -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = - np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1-AL), (1-Y)))/m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2797765635793423"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray([[1, 1, 0]])\n",
    "aL = np.array([[.8,.9,0.4]])\n",
    "compute_cost(aL, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.15171336,  0.06718465, -0.3204696 ,  2.09812712],\n",
       "        [ 0.60345879, -3.72508701,  5.81700741, -3.84326836],\n",
       "        [-0.4319552 , -1.30987417,  1.72354705,  0.05070578],\n",
       "        [-0.38981415,  0.60811244, -1.25938424,  1.47191593],\n",
       "        [-2.52214926,  2.67882552, -0.67947465,  1.48119548]]),\n",
       " array([[ 0.07313866, -0.0976715 , -0.87585828,  0.73763362,  0.00785716],\n",
       "        [ 0.85508818,  0.37530413, -0.59912655,  0.71278189, -0.58931808],\n",
       "        [ 0.97913304, -0.24376494, -0.08839671,  0.55151192, -0.10290907]]),\n",
       " array([[-0.14713786],\n",
       "        [-0.11313155],\n",
       "        [-0.13209101]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dZ = np.random.randn(3,4)\n",
    "A = np.random.randn(5,4)\n",
    "W = np.random.randn(3,5)\n",
    "b = np.random.randn(3,1)\n",
    "linear_cache = (A, W, b)\n",
    "compute_gradients(dZ, linear_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying activation function backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, we should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = compute_gradients(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = compute_gradients(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.44090989,  0.        ],\n",
       "        [ 0.37883606,  0.        ],\n",
       "        [-0.2298228 ,  0.        ]]),\n",
       " array([[ 0.44513824,  0.37371418, -0.10478989]]),\n",
       " array([[-0.20837892]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "dA = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "linear_cache = (A, W, b)\n",
    "activation_cache = Z\n",
    "linear_activation_cache = (linear_cache, activation_cache)\n",
    "\n",
    "apply_activation_backward(dA, linear_activation_cache, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = apply_activation_backward(dAL, caches[L-1], activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = current_cache\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = apply_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], activation=\"relu\")\n",
    "        dA_prev_temp, dW_temp, db_temp = current_cache\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA2': array([[ 0.12913162, -0.44014127],\n",
       "        [-0.14175655,  0.48317296],\n",
       "        [ 0.01663708, -0.05670698]]),\n",
       " 'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]),\n",
       " 'db2': array([[0.15187861]]),\n",
       " 'dA1': array([[ 0.        ,  0.52257901],\n",
       "        [ 0.        , -0.3269206 ],\n",
       "        [ 0.        , -0.32070404],\n",
       "        [ 0.        , -0.74079187]]),\n",
       " 'dW1': array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]),\n",
       " 'db1': array([[-0.22007063],\n",
       "        [ 0.        ],\n",
       "        [-0.02835349]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "AL = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "A1 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "Z1 = np.random.randn(3,2)\n",
    "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "A2 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "Z2 = np.random.randn(1,2)\n",
    "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "backward_propagation(AL, Y, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Updating rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "                \"b1\": b1,\n",
    "                \"W2\": W2,\n",
    "                \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(3,4)\n",
    "db1 = np.random.randn(3,1)\n",
    "dW2 = np.random.randn(1,3)\n",
    "db2 = np.random.randn(1,1)\n",
    "grads = {\"dW1\": dW1,\n",
    "            \"db1\": db1,\n",
    "            \"dW2\": dW2,\n",
    "            \"db2\": db2}\n",
    "\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of data points\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = apply_activation(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = apply_activation(A1, W2, b2, activation=\"sigmoid\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = apply_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = apply_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 4\n",
    "n_h = 3\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931721968423392\n",
      "Cost after iteration 100: 0.6923644035081248\n",
      "Cost after iteration 200: 0.6910076013133749\n",
      "Cost after iteration 300: 0.6872347167095288\n",
      "Cost after iteration 400: 0.6763456670971691\n",
      "Cost after iteration 500: 0.6503203353988694\n",
      "Cost after iteration 600: 0.5999228893026644\n",
      "Cost after iteration 700: 0.515070901877558\n",
      "Cost after iteration 800: 0.42745077398351244\n",
      "Cost after iteration 900: 0.3553770025383742\n",
      "Cost after iteration 1000: 0.28540987498636616\n",
      "Cost after iteration 1100: 0.21073446997527837\n",
      "Cost after iteration 1200: 0.14814454748567127\n",
      "Cost after iteration 1300: 0.10665306974830524\n",
      "Cost after iteration 1400: 0.08001344211042707\n",
      "Cost after iteration 1500: 0.0623338843589064\n",
      "Cost after iteration 1600: 0.0501427043292809\n",
      "Cost after iteration 1700: 0.04142929700117696\n",
      "Cost after iteration 1800: 0.03498759335656611\n",
      "Cost after iteration 1900: 0.030084897621852742\n",
      "Cost after iteration 2000: 0.026259320686524305\n",
      "Cost after iteration 2100: 0.02320958587885577\n",
      "Cost after iteration 2200: 0.02073305823211781\n",
      "Cost after iteration 2300: 0.018689610056176736\n",
      "Cost after iteration 2400: 0.016979880221861962\n",
      "Cost after iteration 2500: 0.015531824127053211\n",
      "Cost after iteration 2600: 0.014292133582096364\n",
      "Cost after iteration 2700: 0.013220640291778446\n",
      "Cost after iteration 2800: 0.012286610618032795\n",
      "Cost after iteration 2900: 0.011466181404017178\n",
      "Cost after iteration 3000: 0.010740576642111185\n",
      "Cost after iteration 3100: 0.010094844446525295\n",
      "Cost after iteration 3200: 0.009516947290538546\n",
      "Cost after iteration 3300: 0.00899709669934853\n",
      "Cost after iteration 3400: 0.008527259892721862\n",
      "Cost after iteration 3500: 0.008100789217815698\n",
      "Cost after iteration 3600: 0.007712140500436973\n",
      "Cost after iteration 3700: 0.007356656624620074\n",
      "Cost after iteration 3800: 0.007030399863723212\n",
      "Cost after iteration 3900: 0.006730019569935836\n",
      "Cost after iteration 4000: 0.0064526481949970654\n",
      "Cost after iteration 4100: 0.006195818918166477\n",
      "Cost after iteration 4200: 0.005957398765934102\n",
      "Cost after iteration 4300: 0.005735534720138393\n",
      "Cost after iteration 4400: 0.0055286098494001\n",
      "Cost after iteration 4500: 0.005335207366639133\n",
      "Cost after iteration 4600: 0.005154080963805134\n",
      "Cost after iteration 4700: 0.00498413025649034\n",
      "Cost after iteration 4800: 0.004824380287275087\n",
      "Cost after iteration 4900: 0.004673964771322844\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = final_model(X_train.T, np.array(y_train).reshape(1, -1), layers_dims = (n_x, n_h, n_y), num_iterations = 5000, \n",
    "                             print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
