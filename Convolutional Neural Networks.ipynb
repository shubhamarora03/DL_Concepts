{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>What we have learnt so far?</font>\n",
    "- We understood what images are? Also looked into medical/dicom images.\n",
    "- We understood different pre-processing steps that are needed to clean image data\n",
    "- We learnt about the computer vision tasks - image classification, object detection, image segmentation etc\n",
    "- We learnt about how traditional machine learning works with image data and how to create hand generated features like - edge detection, computing mean/mode from histograms etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Q1. Why FFNNs can not be used for images?</font>\n",
    "- FFNN does not take into account the `spatial relationship` between the pixels, so it fails to identify the patterns in the image pixels! as shown in below image \n",
    "![alt](convolution5.png)\n",
    "- If there is an object that we are interested in finding in the image, no matter at what position it is present in the image, we can use the same weights to identify that object. This property is known as `parameter sharing`, which also helps to capture the `translation invariance` property of images. We we will see it next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Introducing Convolutional Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class A - Rectange | Class B - Circle\n",
    "- | - \n",
    "![alt](convolution2.png) | ![alt](convolution3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Ignore the dashed lines above in the rectangle, I didn't find any rectangle picture online and assume both of these two images are colored images. What according to you makes these images different from each other? Think from image feature extraction point of view</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>In the above rectange image, there are two `horizontal images`, based on your understanding - what do you think do we need to have two separate filters to identify these two horizontal edges or only one filter can alone identify these two horizontal edges?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution4.gif \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above convolution is known as 2D convolution as the image has only two dimensions. But for an `RGB` the covolution will be a 3D convolution. So what will be the shape of the output when we convolve the colored images of rectange/circle by using 3 different filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the formula to compute output of a convolution operation is -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n + 2p -f}{s} + 1$$\n",
    "we need to `floor` of the above value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where - \n",
    "- n is image size\n",
    "- p is padding\n",
    "- f is filter size\n",
    "- s is stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution6.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Q2. In the above convolution, does all the pixels contributed equally to compute the convolved feature?</font>\n",
    "### <font color='red'>Q3. And what will happen if move the filter faster by taking a long step instead of taking a unit step? Which is also known as `stride`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go back to slide no #62 and #63 and understand this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what `padding` and `stride` is, please try to compute the output shape of convolved feature of the below convolution (assume we not using any padding and unit stride) -\n",
    "![alt text](convolution1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Solution</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolved feature for Class A - rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution7.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolved feature for Class B - circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution9.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging the above filters in proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution10.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Q4. What happens when we have complex objects in the image? The we go `deeper`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution11.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution12.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Q5. If we have 16 filters of dimension 3x3x3, then how many parameters the CNN would have?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Schematic of a CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution13.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Example features learnt by filters - </font>\n",
    "![alt text](convolution16.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Pooling layers</font>\n",
    "Pooling layer helps to progressively `reduce the spatial size of the convolved feature` to **reduce** the amount of `parameters` and `computation` in the network. There are two types of pooling - \n",
    "- Max Pooling\n",
    "- Average Pooling\n",
    "![alt text](convolution14.png \"Title\")\n",
    "![alt text](convolution15.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Fully Connected Layers</font>\n",
    "These are the standard densely connected layer that we have seen before in FFNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Image pre-processing steps - </font>\n",
    "- Removing noise from images e.g. background extraction (we have seen already - Otsu's binarization)\n",
    "- Using normalization in images. Why?\n",
    "- Image augmentation (we will see in a different notebook)\n",
    "- Resizing CNN architecture's required input (we will see in a different notebook)\n",
    "- and many more... depending on the business problem at your hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>How to tune CNN?</font>\n",
    "- filter size\n",
    "- padding\n",
    "- stride\n",
    "- number of filters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
